{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "filled-reliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pke\n",
    "import json\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772ac122",
   "metadata": {},
   "source": [
    "## Change the file path to your current path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-banner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a sample of how to read data and use the dataset\n",
    "import json\n",
    "f = open('./test.json')\n",
    "data = json.load(f)\n",
    "f.close()\n",
    "# data is a dictionary with all of the data \n",
    "for count, body in data.items():\n",
    "    abstract    = body['abstract']\n",
    "    title       = body['title']\n",
    "    kwds_in     = body['keywords_in']\n",
    "    kwds_not_in = body['keywords_not_in']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "spatial-absence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple evaluation method that matches the exact string\n",
    "def match(list1, list2):\n",
    "    same = set(list1).intersection(set(list2))\n",
    "    return len(same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "reverse-examination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "f = open('/home/ubuntu/pubmed/test.json')\n",
    "total = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "incomplete-small",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the result array [@5, @10, @15] stores the result of extracting 5, 10 and 15 keyphrases \n",
    "yake_ground_truth = np.zeros(3)\n",
    "yake_matched = np.zeros(3)\n",
    "yake_extracted = np.zeros(3)\n",
    "\n",
    "# write to file store kwds for extractive methods as a backup \n",
    "f = open('/home/ubuntu/pubmed/extracted_kwds/yakekwds.json', 'w')\n",
    "final = {}\n",
    "\n",
    "# begin extraction\n",
    "for count, body in total.items():\n",
    "    abstract = body['abstract']\n",
    "    kwds_in = body['keywords_in']\n",
    "    kwds_not_in = body['keywords_not_in']\n",
    "    # pke method setup\n",
    "    extractor = pke.unsupervised.YAKE()\n",
    "    extractor.load_document(input=abstract,\n",
    "                            language='en',\n",
    "                            normalization=None)\n",
    "    stoplist = stopwords.words('english')\n",
    "    extractor.candidate_selection(n=3, stoplist=stoplist)\n",
    "    window = 2\n",
    "    use_stems = False # use stems instead of words for weighting\n",
    "    extractor.candidate_weighting(window=window,\n",
    "                                  stoplist=stoplist,\n",
    "                                  use_stems=use_stems)\n",
    "\n",
    "    threshold = 0.8\n",
    "    temp = {5:[], 10:[], 15:[]}\n",
    "    # extract top 5, 10, 15 words\n",
    "    for nn in [5,10,15]:\n",
    "        keyphrases = extractor.get_n_best(n=nn, threshold=threshold)\n",
    "        keyphrases = [i[0] for i in keyphrases]\n",
    "        index = (int)(nn/5-1)\n",
    "        yake_extracted[index] += nn\n",
    "        yake_ground_truth[index] += len(kwds_in)\n",
    "        yake_matched[index] += match(kwds_in, keyphrases)\n",
    "        temp[nn] = keyphrases\n",
    "    final[count] = temp\n",
    "# write to file\n",
    "json.dump(final, f, indent = 4)\n",
    "f.close()\n",
    "\n",
    "# calculate result\n",
    "yake_precision = yake_matched / yake_extracted\n",
    "yake_recall = yake_matched / yake_ground_truth\n",
    "yake_f1 = 2 * yake_precision * yake_recall / (yake_precision + yake_recall)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dd5cdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write result to a file\n",
    "f = open('/home/ubuntu/pubmed/acc.txt','a')\n",
    "f.write(f'yake_precision: {yake_precision}\\n')\n",
    "f.write(f'yake_recall: {yake_recall}\\n')\n",
    "f.write(f'yake_f1: {yake_f1}\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680513c1",
   "metadata": {},
   "source": [
    "## Below are just the same process repeated with other methods in the pke package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-singles",
   "metadata": {},
   "outputs": [],
   "source": [
    "textrank_ground_truth = np.zeros(3)\n",
    "textrank_matched = np.zeros(3)\n",
    "textrank_extracted = np.zeros(3)\n",
    "\n",
    "f = open('/home/ubuntu/pubmed/extracted_kwds/textrankkwds.json', 'w')\n",
    "final = {}\n",
    "\n",
    "for count, body in total.items():\n",
    "    abstract = body['abstract']\n",
    "    kwds_in = body['keywords_in']\n",
    "    kwds_not_in = body['keywords_not_in']\n",
    "    if abstract == \"\":\n",
    "        continue\n",
    "    if kwds_in == [] and kwds_not_in == []:\n",
    "        continue\n",
    "        \n",
    "    pos = {'NOUN', 'PROPN', 'ADJ'}\n",
    "\n",
    "    extractor = pke.unsupervised.TextRank()\n",
    "\n",
    "    extractor.load_document(input=abstract,\n",
    "                            language='en',\n",
    "                            normalization=None)\n",
    "\n",
    "    extractor.candidate_weighting(window=2,\n",
    "                                  pos=pos,\n",
    "                                  top_percent=0.33)\n",
    "    temp = {5:[], 10:[], 15:[]}\n",
    "    for nn in [5,10,15]:\n",
    "        keyphrases = extractor.get_n_best(n=nn)\n",
    "        keyphrases = [i[0] for i in keyphrases]\n",
    "        index = (int)(nn/5-1)\n",
    "        textrank_extracted[index] += nn\n",
    "        textrank_ground_truth[index] += len(kwds_in)\n",
    "        textrank_matched[index] += match(kwds_in, keyphrases)\n",
    "        temp[nn] = keyphrases\n",
    "    final[count] = temp\n",
    "\n",
    "json.dump(final, f, indent = 4)\n",
    "f.close()\n",
    "        \n",
    "textrank_precision = textrank_matched / textrank_extracted\n",
    "textrank_recall = textrank_matched / textrank_ground_truth\n",
    "textrank_f1 = 2 * textrank_precision * textrank_recall / (textrank_precision + textrank_recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20853c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('/home/ubuntu/pubmed/acc.txt','a')\n",
    "f.write(f'textrank_precision: {textrank_precision}\\n')\n",
    "f.write(f'textrank_recall: {textrank_recall}\\n')\n",
    "f.write(f'textrank_f1: {textrank_f1}\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "under-judgment",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "singlerank_ground_truth = np.zeros(3)\n",
    "singlerank_matched = np.zeros(3)\n",
    "singlerank_extracted = np.zeros(3)\n",
    "\n",
    "f = open('/home/ubuntu/pubmed/extracted_kwds/singlerankkwds.json', 'w')\n",
    "final = {}\n",
    "\n",
    "for count, body in total.items():\n",
    "    abstract = body['abstract']\n",
    "    kwds_in = body['keywords_in']\n",
    "    kwds_not_in = body['keywords_not_in']\n",
    "    if abstract == \"\":\n",
    "        continue\n",
    "    if kwds_in == [] and kwds_not_in == []:\n",
    "        continue\n",
    "        \n",
    "    pos = {'NOUN', 'PROPN', 'ADJ'}\n",
    "\n",
    "    extractor = pke.unsupervised.SingleRank()\n",
    "\n",
    "    extractor.load_document(input=abstract,\n",
    "                            language='en',\n",
    "                            normalization=None)\n",
    "\n",
    "    extractor.candidate_selection(pos=pos)\n",
    "\n",
    "    extractor.candidate_weighting(window=10, pos=pos)\n",
    "    temp = {5:[], 10:[], 15:[]}\n",
    "    for nn in [5,10,15]:\n",
    "        keyphrases = extractor.get_n_best(n=nn)\n",
    "        keyphrases = [i[0] for i in keyphrases]\n",
    "        index = (int)(nn/5-1)\n",
    "        singlerank_extracted[index] += nn\n",
    "        singlerank_ground_truth[index] += len(kwds_in)\n",
    "        singlerank_matched[index] += match(kwds_in, keyphrases)\n",
    "        temp[nn] = keyphrases\n",
    "    final[count] = temp\n",
    "\n",
    "json.dump(final, f, indent = 4)\n",
    "f.close()\n",
    "                \n",
    "singlerank_precision = singlerank_matched / singlerank_extracted\n",
    "singlerank_recall = singlerank_matched / singlerank_ground_truth\n",
    "singlerank_f1 = 2 * singlerank_precision * singlerank_recall / (singlerank_precision + singlerank_recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "flexible-parker",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('/home/ubuntu/pubmed/acc.txt','a')\n",
    "f.write(f'singlerank_precision: {singlerank_precision}\\n')\n",
    "f.write(f'singlerank_recall: {singlerank_recall}\\n')\n",
    "f.write(f'singlerank_f1: {singlerank_f1}\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "latter-covering",
   "metadata": {},
   "outputs": [],
   "source": [
    "topicrank_ground_truth = np.zeros(3)\n",
    "topicrank_matched = np.zeros(3)\n",
    "topicrank_extracted = np.zeros(3)\n",
    "\n",
    "f = open('/home/ubuntu/pubmed/extracted_kwds/topicrankkwds.json', 'w')\n",
    "final = {}\n",
    "\n",
    "for count, body in total.items():\n",
    "    abstract = body['abstract']\n",
    "    kwds_in = body['keywords_in']\n",
    "    kwds_not_in = body['keywords_not_in']\n",
    "    if abstract == \"\":\n",
    "        continue\n",
    "    if kwds_in == [] and kwds_not_in == []:\n",
    "        continue\n",
    "        \n",
    "    extractor = pke.unsupervised.TopicRank()\n",
    "    extractor.load_document(input=abstract)\n",
    "\n",
    "    pos = {'NOUN', 'PROPN', 'ADJ'}\n",
    "    stoplist = list(string.punctuation)\n",
    "    stoplist += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n",
    "    stoplist += stopwords.words('english')\n",
    "    extractor.candidate_selection(pos=pos, stoplist=stoplist)\n",
    "\n",
    "    extractor.candidate_weighting(threshold=0.74, method='average')\n",
    "    temp = {5:[], 10:[], 15:[]}\n",
    "    for nn in [5,10,15]:\n",
    "        keyphrases = extractor.get_n_best(n=nn)\n",
    "        keyphrases = [i[0] for i in keyphrases]\n",
    "        index = (int)(nn/5-1)\n",
    "        topicrank_extracted[index] += nn\n",
    "        topicrank_ground_truth[index] += len(kwds_in)\n",
    "        topicrank_matched[index] += match(kwds_in, keyphrases)\n",
    "        temp[nn] = keyphrases\n",
    "    final[count] = temp\n",
    "\n",
    "json.dump(final, f, indent = 4)\n",
    "f.close()        \n",
    "topicrank_precision = topicrank_matched / topicrank_extracted\n",
    "topicrank_recall = topicrank_matched / topicrank_ground_truth\n",
    "topicrank_f1 = 2 * topicrank_precision * topicrank_recall / (topicrank_precision + topicrank_recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236a0ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('/home/ubuntu/pubmed/acc.txt','a')\n",
    "f.write(f'topicrank_precision: {topicrank_precision}\\n')\n",
    "f.write(f'topicrank_recall: {topicrank_recall}\\n')\n",
    "f.write(f'topicrank_f1: {topicrank_f1}\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6841888",
   "metadata": {},
   "outputs": [],
   "source": [
    "positionrank_ground_truth = np.zeros(3)\n",
    "positionrank_matched = np.zeros(3)\n",
    "positionrank_extracted = np.zeros(3)\n",
    "\n",
    "f = open('/home/ubuntu/pubmed/extracted_kwds/positionrankkwds.json', 'w')\n",
    "final = {}\n",
    "\n",
    "for count, body in total.items():\n",
    "    abstract = body['abstract']\n",
    "    kwds_in = body['keywords_in']\n",
    "    kwds_not_in = body['keywords_not_in']\n",
    "    if abstract == \"\":\n",
    "        continue\n",
    "    if kwds_in == [] and kwds_not_in == []:\n",
    "        continue\n",
    "    pos = {'NOUN', 'PROPN', 'ADJ'}\n",
    "\n",
    "    grammar = \"NP: {<ADJ>*<NOUN|PROPN>+}\"\n",
    "\n",
    "    extractor = pke.unsupervised.PositionRank()\n",
    "    extractor.load_document(input=abstract,\n",
    "                            language='en',\n",
    "                            normalization=None)\n",
    "\n",
    "    extractor.candidate_selection(grammar=grammar,\n",
    "                                  maximum_word_number=3)\n",
    "\n",
    "    extractor.candidate_weighting(window=10, pos=pos)\n",
    "    temp = {5:[], 10:[], 15:[]}\n",
    "    for nn in [5,10,15]:\n",
    "        keyphrases = extractor.get_n_best(n=nn)\n",
    "        keyphrases = [i[0] for i in keyphrases]\n",
    "        index = (int)(nn/5-1)\n",
    "        positionrank_extracted[index] += nn\n",
    "        positionrank_ground_truth[index] += len(kwds_in)\n",
    "        positionrank_matched[index] += match(kwds_in, keyphrases)\n",
    "        temp[nn] = keyphrases\n",
    "    final[count] = temp\n",
    "\n",
    "json.dump(final, f, indent = 4)\n",
    "f.close()          \n",
    "positionrank_precision = positionrank_matched / positionrank_extracted\n",
    "positionrank_recall = positionrank_matched / positionrank_ground_truth\n",
    "positionrank_f1 = 2 * positionrank_precision * positionrank_recall / (positionrank_precision + positionrank_recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f31c5194",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('/home/ubuntu/pubmed/acc.txt','a')\n",
    "f.write(f'positionrank_precision: {positionrank_precision}\\n')\n",
    "f.write(f'positionrank_recall: {positionrank_recall}\\n')\n",
    "f.write(f'positionrank_f1: {positionrank_f1}\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "844254b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "multirank_ground_truth = np.zeros(3)\n",
    "multirank_matched = np.zeros(3)\n",
    "multirank_extracted = np.zeros(3)\n",
    "\n",
    "f = open('/home/ubuntu/pubmed/extracted_kwds/multirankkwds.json', 'w')\n",
    "final = {}\n",
    "\n",
    "for count, body in total.items():\n",
    "    abstract = body['abstract']\n",
    "    kwds_in = body['keywords_in']\n",
    "    kwds_not_in = body['keywords_not_in']\n",
    "    if abstract == \"\":\n",
    "        continue\n",
    "    if kwds_in == [] and kwds_not_in == []:\n",
    "        continue\n",
    "    extractor = pke.unsupervised.MultipartiteRank()\n",
    "\n",
    "    extractor.load_document(input=abstract)\n",
    "\n",
    "    pos = {'NOUN', 'PROPN', 'ADJ'}\n",
    "    stoplist = list(string.punctuation)\n",
    "    stoplist += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n",
    "    stoplist += stopwords.words('english')\n",
    "    extractor.candidate_selection(pos=pos, stoplist=stoplist)\n",
    "\n",
    "    extractor.candidate_weighting(alpha=1.1,\n",
    "                                  threshold=0.74,\n",
    "                                  method='average')\n",
    "    temp = {5:[], 10:[], 15:[]}\n",
    "    for nn in [5,10,15]:\n",
    "        keyphrases = extractor.get_n_best(n=nn)\n",
    "        keyphrases = [i[0] for i in keyphrases]\n",
    "        index = (int)(nn/5-1)\n",
    "        multirank_extracted[index] += nn\n",
    "        multirank_ground_truth[index] += len(kwds_in)\n",
    "        multirank_matched[index] += match(kwds_in, keyphrases)\n",
    "        temp[nn] = keyphrases\n",
    "    final[count] = temp\n",
    "\n",
    "json.dump(final, f, indent = 4)\n",
    "f.close()                  \n",
    "multirank_precision = multirank_matched / multirank_extracted\n",
    "multirank_recall = multirank_matched / multirank_ground_truth\n",
    "multirank_f1 = 2 * multirank_precision * multirank_recall / (multirank_precision + multirank_recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8265a94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('/home/ubuntu/pubmed/acc.txt','a')\n",
    "f.write(f'multirank_precision: {multirank_precision}\\n')\n",
    "f.write(f'multirank_recall: {multirank_recall}\\n')\n",
    "f.write(f'multirank_f1: {multirank_f1}\\n')\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p37)",
   "language": "python",
   "name": "conda_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
